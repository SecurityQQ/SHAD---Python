{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import urllib.request as r\n",
    "import bs4\n",
    "import re\n",
    "url = 'http://hip-hop.name/text/guf/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# WARNING: Я не смог быть адекватным в этом задании и решил выкачать треки русского рэпера"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "response = r.urlopen(url)\n",
    "html_doc = response.read()\n",
    "soup = bs4.BeautifulSoup(html_doc, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "links = []\n",
    "for link in soup.find_all('a'):\n",
    "    links.append(link.get('href'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "links = [*filter(lambda x: x is not None and x[:4] == '/guf', links)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "texts_urls = ['http://hip-hop.name' + link for link in links]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.03 s, sys: 140 ms, total: 4.17 s\n",
      "Wall time: 23.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "learning_text = ''\n",
    "for url in texts_urls:\n",
    "    try:\n",
    "        response = r.urlopen(url)\n",
    "        html_doc = response.read()\n",
    "        soup = bs4.BeautifulSoup(html_doc, 'html.parser')\n",
    "        s = soup.find_all('div', 'entry')[0]\n",
    "        learning_text += s.get_text()\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "import string\n",
    "from collections import Counter, OrderedDict\n",
    "import random\n",
    "import argparse\n",
    "import sys\n",
    "import unittest\n",
    "\n",
    "\n",
    "class Utils(object):\n",
    "\n",
    "    @staticmethod\n",
    "    def split_symbol(text, symbol, drop_symbol=False):\n",
    "        if drop_symbol:\n",
    "            return text.split(symbol)\n",
    "\n",
    "        parts = text.split(symbol)\n",
    "        result = []\n",
    "        for part in parts:\n",
    "            result += [part, symbol]\n",
    "        return result[:-1]\n",
    "\n",
    "    @staticmethod\n",
    "    def split_for_tokens(text):\n",
    "        splited = []\n",
    "        number = ''\n",
    "        word = ''\n",
    "        other = ''\n",
    "        for i in range(len(text)):\n",
    "            if text[i].isdigit():\n",
    "                if len(other) > 0:\n",
    "                    splited += [other]\n",
    "                    other = ''\n",
    "                if len(word) > 0:\n",
    "                    splited += [word]\n",
    "                    word = ''\n",
    "                number += text[i]\n",
    "            elif text[i].isalpha():\n",
    "                if len(other) > 0:\n",
    "                    splited += [other]\n",
    "                    other = ''\n",
    "                if len(number) > 0:\n",
    "                    splited += [number]\n",
    "                    number = ''\n",
    "                word += text[i]\n",
    "            else:\n",
    "                if len(word) > 0:\n",
    "                    splited += [word]\n",
    "                    word = ''\n",
    "                if len(number) > 0:\n",
    "                    splited += [number]\n",
    "                    number = ''\n",
    "                other += text[i]\n",
    "        if len(word) > 0:\n",
    "            splited += [word]\n",
    "        if len(number) > 0:\n",
    "            splited += [number]\n",
    "        if len(other) > 0:\n",
    "            splited += [other]\n",
    "        return splited\n",
    "\n",
    "    @staticmethod\n",
    "    def parts_split_symbol(parts, symbol, drop_symbol=False):\n",
    "        new_parts = []\n",
    "        for part in parts:\n",
    "            new_parts += Utils.split_symbol(part, symbol, drop_symbol)\n",
    "        return [s for s in filter(lambda x: len(x) != 0, new_parts)]\n",
    "\n",
    "    @staticmethod\n",
    "    def split(text, delimiters, delimiters_to_drop=[]):\n",
    "        \"\"\"\n",
    "\n",
    "        :param text: type(str)\n",
    "        :param delimiters: to split text\n",
    "        :param delimiters_to_drop: to split text and not include to result\n",
    "        :return: splitted text with delimiters and w/o delimiters_to_drop\n",
    "        \"\"\"\n",
    "        parts = [text]\n",
    "\n",
    "        for symbol in delimiters_to_drop:\n",
    "            parts = Utils.parts_split_symbol(parts, symbol, drop_symbol=True)\n",
    "\n",
    "        for symbol in delimiters:\n",
    "            parts = Utils.parts_split_symbol(parts, symbol, drop_symbol=False)\n",
    "\n",
    "        new_parts = []\n",
    "        for part in parts:\n",
    "            new_parts += Utils.split_for_tokens(part)\n",
    "        return new_parts\n",
    "\n",
    "    @staticmethod\n",
    "    def tokenize(text, depth=1, drop_whitespace=False):\n",
    "        \"\"\"\n",
    "\n",
    "        :param text: type(str)\n",
    "        :param depth: length of words chain\n",
    "        :param drop_whitespace: if true, drops whitespaces in final list\n",
    "        :return: list of tokens\n",
    "        \"\"\"\n",
    "        delimiters = []\n",
    "        delimiters_to_drop = []\n",
    "        if drop_whitespace:\n",
    "            delimiters = []\n",
    "            delimiters_to_drop = string.whitespace + string.punctuation\n",
    "        else:\n",
    "            delimiters = string.punctuation + string.whitespace\n",
    "\n",
    "        splited = Utils.split(text, delimiters, delimiters_to_drop)\n",
    "        return [tuple(splited[i:i + depth]) for i in range(0, len(splited) - depth + 1)]\n",
    "\n",
    "    @staticmethod\n",
    "    def get_chains(text, depth=1):\n",
    "        splited_text = text.split('\\n')\n",
    "        chains = dict()\n",
    "        for line in splited_text:\n",
    "            chains_with_future = Utils.tokenize(line, depth + 1, drop_whitespace=True)\n",
    "            for item in chains_with_future:\n",
    "                if item[:depth] not in chains:\n",
    "                    chains[item[:depth]] = []\n",
    "                chains[item[:depth]] += item[depth:]\n",
    "        return chains\n",
    "\n",
    "    @staticmethod\n",
    "    def count_probabilities(chains):\n",
    "        chains_with_probabilities = OrderedDict()\n",
    "        for key, values in chains.items():\n",
    "            counts = Counter(values)\n",
    "            total = len(values)\n",
    "            probabilities = OrderedDict()\n",
    "            for v in values:\n",
    "                probabilities[v] = counts[v] / total\n",
    "            chains_with_probabilities[key] = OrderedDict(sorted(list(probabilities.items()),\n",
    "                                                                key=lambda x: x[0])\n",
    "                                                         )\n",
    "        return OrderedDict(sorted(chains_with_probabilities.items(), key=lambda x: x[0]))\n",
    "\n",
    "    @staticmethod\n",
    "    def unique(l):\n",
    "        \"\"\"\n",
    "        :param l:\n",
    "        :return: unique elements of list\n",
    "        \"\"\"\n",
    "        return list(set(l))\n",
    "\n",
    "\n",
    "class TokenizeTask(object):\n",
    "    def __init__(self, text):\n",
    "        self.text = text\n",
    "        self.tokens = Utils.tokenize(self.text)\n",
    "\n",
    "    def __str__(self):\n",
    "        return '\\n'.join([s[0] for s in self.tokens])\n",
    "\n",
    "\n",
    "class CalculateProbabilitiesTask(object):\n",
    "    def __init__(self, text, depth=1):\n",
    "        self.chains = [Utils.get_chains(text, depth=i) for i in range(0, depth + 1)]\n",
    "        self.probabilities = [Utils.count_probabilities(chain) for chain in self.chains]\n",
    "\n",
    "    def __str__(self):\n",
    "        pr = []\n",
    "        for d in self.probabilities:\n",
    "            for key, values in d.items():\n",
    "                pr += [(key, values)]\n",
    "        pr = sorted(pr, key=lambda x: x[0])\n",
    "        ans = []\n",
    "        for pr_per_level in pr:\n",
    "            history, values = pr_per_level[0], pr_per_level[1]\n",
    "            ans.append(' '.join(history))\n",
    "            for next_word, p in values.items():\n",
    "                ans.append('  {}: {:.2f}'.format(next_word, p))\n",
    "        return '\\n'.join(ans)\n",
    "\n",
    "\n",
    "class TextGenerator(object):\n",
    "    def __init__(self, depth=1, size=20):\n",
    "        self.depth = depth\n",
    "        self.size = size\n",
    "        self.probabilities = []\n",
    "\n",
    "    def fit(self, text):\n",
    "        prob_task = CalculateProbabilitiesTask(text, self.depth)\n",
    "        self.probabilities = prob_task.probabilities\n",
    "\n",
    "    def generate(self):\n",
    "        generated_words = []\n",
    "        new_word = self.__choice(self.probabilities[0][()])\n",
    "        generated_words.append(new_word)\n",
    "\n",
    "        for i in range(1, self.size):\n",
    "            slice_size = min(self.depth - 1, i)\n",
    "            prev_chain = tuple(generated_words[-slice_size:])\n",
    "            while prev_chain not in self.probabilities[slice_size]:\n",
    "                slice_size -= 1\n",
    "                prev_chain = tuple(generated_words[-slice_size:])\n",
    "                if (slice_size < 0):\n",
    "                    slice_size = 0\n",
    "                    prev_chain = ()\n",
    "                    break\n",
    "            new_word = self.__choice(self.probabilities[slice_size][prev_chain])\n",
    "            generated_words.append(new_word)\n",
    "        generated_words[0] = generated_words[0].capitalize()\n",
    "        return ' '.join(generated_words)\n",
    "\n",
    "    def __choice(self, future):\n",
    "        total_prob = 0\n",
    "        for next_word, p in future.items():\n",
    "            total_prob += p\n",
    "        result = random.uniform(0, total_prob)\n",
    "        lower_bound = 0\n",
    "        for next_word, p in future.items():\n",
    "            lower_bound += p\n",
    "            if lower_bound > result:\n",
    "                return next_word\n",
    "\n",
    "\n",
    "class UnitTest(unittest.TestCase):\n",
    "    @staticmethod\n",
    "    def test_tokenize():\n",
    "        res = TokenizeTask('1234 5678, 9101112! We love BMW and you love')\n",
    "        answer = [\n",
    "                    '1234',\n",
    "                    ' ',\n",
    "                    '5678',\n",
    "                    ',',\n",
    "                    ' ',\n",
    "                    '9101112',\n",
    "                    '!',\n",
    "                    ' ',\n",
    "                    'We',\n",
    "                    ' ',\n",
    "                    'love',\n",
    "                    ' ',\n",
    "                    'BMW',\n",
    "                    ' ',\n",
    "                    'and',\n",
    "                    ' ',\n",
    "                    'you',\n",
    "                    ' ',\n",
    "                    'love'\n",
    "                ]\n",
    "        assert([x[0] for x in res.tokens] == answer)\n",
    "\n",
    "    @staticmethod\n",
    "    def test_probabilities_calculation():\n",
    "        result = CalculateProbabilitiesTask('First test Second test', depth=1).__str__().split()\n",
    "        answer = ['First:', '0.25',\n",
    "                  'Second:', '0.25',\n",
    "                  'test:', '0.50',\n",
    "                  'First', 'test:', '1.00',\n",
    "                  'Second', 'test:', '1.00',\n",
    "                  'test', 'Second:', '1.00']\n",
    "        assert(result == answer)\n",
    "\n",
    "    @staticmethod\n",
    "    def test_generation():\n",
    "        try:\n",
    "            generator = TextGenerator(depth=3, size=5)\n",
    "            generator.fit(\"а блоки, каждый блок соответствует цепочке истории\")\n",
    "            generator.generate()\n",
    "        except _:\n",
    "            assert False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "tg = TextGenerator(5, 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "239309"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(learning_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "tg.fit(learning_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Самое веселое (жестокое) (...Какого черта вообще мне это пришло в голову?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Потерялся \n",
      "сам где Клязьма а где у \n",
      "нас Яуза в батлах ведь мы \n",
      "тут а не там Монте Карло \n",
      "играет нормальная песня но еще не \n",
      "закончил в Окидоге в итоге в \n",
      "правой руке телефон Nokia Слышь качок \n",
      "что ловите про бабулю даже на \n",
      "гастролях зимой шарф с того дня \n",
      "как то странно но походу правильно \n",
      "писать « 2 pac » наверно \n",
      "сейчас немного расстрою мэйл и дуем \n",
      "семь утра во вторник я иду \n",
      "на концерт группы корни » маэстро \n",
      "валит законами жанра мне доступ на \n",
      "эту крышу фонарь так уже знают \n",
      "что такое ZM меня поймут как \n",
      "он нужен тут на этой студии \n",
      "к девяти утра в свою пыльную \n",
      "конторку ветерок куража хорошей ганжи мне \n",
      "не станет хуже и всё глубже \n",
      "И я вчера всех видеть рад \n",
      "был очередной наглый табор Кто первый \n",
      "Ну скажем Виагра Да ну без \n",
      "мазы я уже там был за \n",
      "то что не пришёл листаю кого \n",
      "бы еще удалить из контакта на \n",
      "студийке запишем новых песен и стихов \n",
      "про дружбу и в пожар и \n",
      "в воду тем кто с нами \n",
      "делится чудесами Боже дай сил запомнить \n",
      "эти города не спят большие люди \n",
      "тоже не надо забывать что пока \n",
      "есть такой чел в эту систему \n",
      "вхожий "
     ]
    }
   ],
   "source": [
    "rap_gad = tg.generate()\n",
    "l = rap_gad.split(' ')\n",
    "for i in range(0, len(l)):\n",
    "    print(l[i], end=' ')\n",
    "    if i % 6 == 0:\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Тамара \n",
      "Константиновна конверт что умею унижать брата \n",
      "от брата напрасно У меня аритмия \n",
      "иногда у меня вообще не бьётся \n",
      "сердце острыми шипами конторой получится надеюсь \n",
      "выйдет круто и нереально красивая стену \n",
      "Вася по классике заваривает в тазике \n",
      "что то китайское руке fuck yeah \n",
      "” прятать от родителей И тусуйтесь \n",
      "там суетититесь любуйтесь И чуткие органы \n",
      "правосудия тут не дошуток дело такое \n",
      "бо бо бо бо бо бо \n",
      "бо бо бо бо бо бо \n",
      "бо бо бо бо бо бо \n",
      "бо бо бо бо бо бо \n",
      "бо бо бо бо бо бо \n",
      "бо бо бо бо бо бо \n",
      "бо бо бо бо бо бо \n",
      "бо бо бо бо бо бо \n",
      "бо бо бо бо бо бо \n",
      "бо бо бо бо бо бо \n",
      "бо бо бо бо бо бо \n",
      "бо бо бо бо бо бо \n",
      "бо бо бо бо бо бо \n",
      "бо бо бо бо бо бо \n",
      "бо бо бо бо бо бо \n",
      "бо бо бо бо бо бо \n",
      "бо бо бо бо бо бо \n",
      "бо бо бо бо бо бо \n",
      "бо бо бо бо бо бо \n",
      "бо бо бо бо бо бо \n",
      "бо бо бо бо бо бо \n",
      "бо бо бо бо бо бо \n",
      "бо бо бо бо бо бо \n",
      "бо "
     ]
    }
   ],
   "source": [
    "rap_gad = tg.generate()\n",
    "l = rap_gad.split(' ')\n",
    "for i in range(0, len(l)):\n",
    "    print(l[i], end=' ')\n",
    "    if i % 6 == 0:\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Весь \n",
      "этот замес из за денег на \n",
      "трубу закинул как никотин … алко \n",
      "…  и грусть … я знаю \n",
      "съезд без ДПС вкусом свободы салоне \n",
      "даги офисе не жадный но из \n",
      "за погоды хотелось грустить Не помню \n",
      "то ли сон какой то приснился \n",
      "трудно пустить самокрутку по кругу я \n",
      "снова буду делать круто оказалось просто \n",
      "мутно стаффа меньше табака друзья до \n",
      "единого конченные дебилы хорошо убило С \n",
      "ним бывает такое музло это не \n",
      "рэп » напевают с нами ещё \n",
      "Плюх бух людей в форме спросить \n",
      "« Который час » и много \n",
      "смотрю из окна все та же \n",
      "высота поделим брат ты мне не \n",
      "подельник Скручиваю взрываю думаю — нахуй \n",
      "все это надо же что я \n",
      "слышу  — давай ебашь уже раз \n",
      "ебашишь Но тем не менее размеренным \n",
      "Но я рифмую и пропускаю мимо \n",
      "ушей их треп они все как \n",
      "один меня посылают на студии перезимуем \n",
      "здесь не мёрзнем то что несём \n",
      "доступно многим себя меньше вальса больше \n",
      "гопака тут кормят как это обычно \n",
      "бывает скромно три типа стояли скромно \n",
      "бухали во дворе у меня нет \n",
      "наркотикам возвращаться такие маршруты никто из \n",
      "троих толком не говорит нифига не \n",
      "должен в этом разбираться лучше ну \n",
      "давай наяривай гитара 7 ми струнная \n",
      "ты "
     ]
    }
   ],
   "source": [
    "rap_gad = tg.generate()\n",
    "l = rap_gad.split(' ')\n",
    "for i in range(0, len(l)):\n",
    "    print(l[i], end=' ')\n",
    "    if i % 6 == 0:\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
