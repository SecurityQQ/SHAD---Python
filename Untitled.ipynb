{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import urllib.request as r\n",
    "import bs4\n",
    "import re\n",
    "url = 'http://hip-hop.name/text/guf/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WARNING: Я не смог быть адекватным в этом задании и решил выкачать треки русского рэпера"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "response = r.urlopen(url)\n",
    "html_doc = response.read()\n",
    "soup = bs4.BeautifulSoup(html_doc, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "links = []\n",
    "for link in soup.find_all('a'):\n",
    "    links.append(link.get('href'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "links = [*filter(lambda x: x is not None and x[:4] == '/guf', links)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "texts_urls = ['http://hip-hop.name' + link for link in links]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "learning_text = ''\n",
    "for url in texts_urls:\n",
    "    try:\n",
    "        response = r.urlopen(url)\n",
    "        html_doc = response.read()\n",
    "        soup = bs4.BeautifulSoup(html_doc, 'html.parser')\n",
    "        s = soup.find_all('div', 'entry')[0]\n",
    "        learning_text += s.get_text()\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import string\n",
    "from collections import Counter, OrderedDict\n",
    "import bs4\n",
    "import random\n",
    "\n",
    "\n",
    "class Utils(object):\n",
    "\n",
    "    @staticmethod\n",
    "    def split_symbol(text, symbol, drop_symbol=False):\n",
    "        if drop_symbol:\n",
    "            return text.split(symbol)\n",
    "\n",
    "        parts = text.split(symbol)\n",
    "        result = []\n",
    "        for part in parts:\n",
    "            result += [part, symbol]\n",
    "        return result[:-1]\n",
    "\n",
    "    @staticmethod\n",
    "    def parts_split_symbol(parts, symbol, drop_symbol=False):\n",
    "        new_parts = []\n",
    "        for part in parts:\n",
    "            new_parts += Utils.split_symbol(part, symbol, drop_symbol)\n",
    "        return [*filter(lambda x: len(x) != 0, new_parts)]\n",
    "\n",
    "    @staticmethod\n",
    "    def split(text, delimiters, delimiters_to_drop=[]):\n",
    "        \"\"\"\n",
    "\n",
    "        :param text: type(str)\n",
    "        :param delimiters: to split text\n",
    "        :param delimiters_to_drop: to split text and not include to result\n",
    "        :return: splitted text with delimiters and w/o delimiters_to_drop\n",
    "        \"\"\"\n",
    "        parts = [text]\n",
    "\n",
    "        for symbol in delimiters_to_drop:\n",
    "            parts = Utils.parts_split_symbol(parts, symbol, drop_symbol=True)\n",
    "\n",
    "        for symbol in delimiters:\n",
    "            parts = Utils.parts_split_symbol(parts, symbol, drop_symbol=False)\n",
    "\n",
    "        return parts\n",
    "\n",
    "    @staticmethod\n",
    "    def tokenize(text, depth=1, drop_whitespace=False):\n",
    "        \"\"\"\n",
    "\n",
    "        :param text: type(str)\n",
    "        :param depth: length of words chain\n",
    "        :param drop_whitespace: if true, drops whitespaces in final list\n",
    "        :return: list of tokens\n",
    "        \"\"\"\n",
    "        delimiters = []\n",
    "        delimiters_to_drop = []\n",
    "        if drop_whitespace:\n",
    "            delimiters = string.punctuation\n",
    "            delimiters_to_drop = string.whitespace\n",
    "        else:\n",
    "            delimiters = string.punctuation + string.whitespace\n",
    "\n",
    "        splited = Utils.split(text, delimiters, delimiters_to_drop)\n",
    "        return [tuple(splited[i:i + depth]) for i in range(0, len(splited) - depth + 1)]\n",
    "\n",
    "    @staticmethod\n",
    "    def get_chains(text, depth=1):\n",
    "        chains_with_future = Utils.tokenize(text, depth + 1, drop_whitespace=True)\n",
    "        chains = dict()\n",
    "        for item in chains_with_future:\n",
    "            if item[:depth] not in chains:\n",
    "                chains[item[:depth]] = []\n",
    "            chains[item[:depth]] += item[depth:]\n",
    "        return chains\n",
    "\n",
    "    @staticmethod\n",
    "    def count_probabilities(chains):\n",
    "        chains_with_probabilities = OrderedDict()\n",
    "        for key, values in chains.items():\n",
    "            counts = Counter(values)\n",
    "            total = len(values)\n",
    "            probabilities = OrderedDict()\n",
    "            for v in values:\n",
    "                probabilities[v] = counts[v] / total\n",
    "            chains_with_probabilities[key] = OrderedDict(sorted(list(probabilities.items()), key=lambda x: x[0]))\n",
    "        return OrderedDict(sorted(chains_with_probabilities.items(), key=lambda x: x[0]))\n",
    "\n",
    "    @staticmethod\n",
    "    def unique(l):\n",
    "        \"\"\"\n",
    "        :param l:\n",
    "        :return: unique elements of list\n",
    "        \"\"\"\n",
    "        return list(set(l))\n",
    "\n",
    "class TokenizeTask(object):\n",
    "    def __init__(self, text):\n",
    "        self.text = text\n",
    "        self.tokens = Utils.tokenize(self.text)\n",
    "\n",
    "    def __str__(self):\n",
    "       return '\\n'.join([s[0] for s in self.tokens])\n",
    "\n",
    "\n",
    "class CalculateProbabilitiesTask(object):\n",
    "    def __init__(self, text, depth=1):\n",
    "        self.text = text.replace('\\n', '')\n",
    "        self.chains = [Utils.get_chains(text, depth=i) for i in range(0, depth + 1)]\n",
    "        self.probabilities = [Utils.count_probabilities(chain) for chain in self.chains]\n",
    "\n",
    "    def __str__(self):\n",
    "        ans = []\n",
    "        for pr_per_level in self.probabilities:\n",
    "            for history, future in pr_per_level.items():\n",
    "                if len(history) != 0:\n",
    "                    ans.append(' '.join(history))\n",
    "                for next_word, p in future.items():\n",
    "                    ans.append('  {}: {:0.2}'.format(next_word, p))\n",
    "        return '\\n'.join(ans)\n",
    "\n",
    "\n",
    "\n",
    "class TextGenerator(object):\n",
    "    def __init__(self, depth=1, size=20):\n",
    "        self.depth = depth\n",
    "        self.size = size\n",
    "        self.probabilities = []\n",
    "\n",
    "    def fit(self, text):\n",
    "        prob_task = CalculateProbabilitiesTask(text, self.depth)\n",
    "        self.probabilities = prob_task.probabilities\n",
    "\n",
    "    def generate(self):\n",
    "        generated_words = []\n",
    "        new_word = self.__choice(self.probabilities[0][()])\n",
    "        generated_words.append(new_word)\n",
    "\n",
    "        for i in range(1, self.size):\n",
    "            slice_size = random.randint(1, min(self.depth - 1, i))\n",
    "            prev_chain = tuple(generated_words[-slice_size:])\n",
    "            while prev_chain not in self.probabilities[slice_size]:\n",
    "                slice_size -= 1\n",
    "                prev_chain = tuple(generated_words[-slice_size:])\n",
    "\n",
    "            new_word = self.__choice(self.probabilities[slice_size][prev_chain])\n",
    "            generated_words.append(new_word)\n",
    "        generated_words[0] = generated_words[0].capitalize()\n",
    "        return ' '.join(generated_words)\n",
    "\n",
    "    def __choice(self, future):\n",
    "        total_prob = 0\n",
    "        for next_word, p in future.items():\n",
    "            total_prob += p\n",
    "        result = random.uniform(0, total_prob)\n",
    "        lower_bound = 0\n",
    "        for next_word, p in future.items():\n",
    "            lower_bound += p\n",
    "            if lower_bound > result:\n",
    "                return next_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tg = TextGenerator(10, 600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "239309"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(learning_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tg.fit(learning_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Улицам \n",
      "спокойно , сквозь шум и суету \n",
      "И помни каким миром , Может \n",
      "быть наполнена тишина . Насколько можешь \n",
      "неустанно Поддерживать добрые отношение со всеми \n",
      ", Везде и СПИД позже ? \n",
      "Слушай сюда , если дашь ему \n",
      "забить косяк , когда вас много \n",
      ", Не поднимаюсь на любимую Крышу \n",
      "даже по воскресениям , Не выступаю \n",
      "с предложениями в Московской мэрии , \n",
      "А хожу по Замоскворечью . Встретить \n",
      "красную шапочку , на кого я \n",
      "похож . Таких динозавров как ! \n",
      "А водка ? — Литр выпьешь \n",
      "? — Не смогу сделать и \n",
      "глотка ! — Ну как ? \n",
      "Рад ? — Что ? — \n",
      "Ну да , стоп , тормоз \n",
      ", Взрывай , задуй паровоз , \n",
      "война , Марсово поле , пили \n",
      "Баварию , Считали трамваи и пусть \n",
      "еще не довольны чем - то \n",
      "И я до сих пор утверждает \n",
      ", Что со всей рэп - \n",
      "движухой повезло мне . И кто \n",
      "- то наркотическими . Это закончится \n",
      "очень скоро , А пока в \n",
      "моей голове не дают уснуть . \n",
      "Демоны снуют и кишат . Не \n",
      "стоит постоянно сравнивать себя с другими \n",
      ", Ты рискуешь стать очень похожи \n",
      "Всего одна любовь , одна на \n",
      "эту крышу . Я шел на \n",
      "площадку , когда было жарко , \n",
      "Если начиналась гроза , я прятался \n",
      "в арке , Ждал появления солнца \n",
      ", смотрел на ухо , ибо \n",
      "они пытка для мс Перезимуем на \n",
      "студийке , запишем это вместе Холода \n",
      "не беда для мс Перезимуем на \n",
      "студийке , запишем новых песен Холода \n",
      "не считает Надежд не теряет Если \n",
      "бьёт — то на проспектах , Тем \n",
      ", Почему не стали подавать заявление \n",
      "в милицию , Все втроем мы \n",
      "начали звонить в полете боинг Как \n",
      "воин перед боем , спокоен как \n",
      "Как ветер на горном склоне Как \n",
      "лотос в гармонии , как подставлять \n",
      "Дружба учит ещё и прощать . \n",
      "Истории из жизни – мелкие обрывки \n",
      "памяти . Ни хапки с утра \n",
      ". Sorry , братан . Мир \n",
      ". На сотню москвичей , всего \n",
      "один родился тут . Guf — три \n",
      "буквы , я жутко мутный , \n",
      "Вась , замес лютый , ты \n",
      "смотри не попутай . В тёмное \n",
      "время фоткают странные люди в подъезде \n",
      ". Я здесь , как обычно \n",
      ", скромно . . . От \n",
      "мавзолея налево , через мост , \n",
      "Это — район , салам алейкум , \n",
      "все очень просто , Мы — дома \n",
      ", кому - то пишем и \n",
      "это от души , Мы этим \n",
      "счастливы . Из глубин сознания , \n",
      "Из самых его углов , Ещё \n",
      "того липкого стаффа нужен примерно корабль \n",
      ", Ну ведь не даром мы \n",
      "с ним наводили движухи Вы что \n",
      "- то походу перепутали — вас наебнули \n",
      ". Может это вымысел ? Я \n",
      "пока не выяснил , Но есть \n",
      "некая сила в стиле , два \n",
      ", три , четыре , БиС \n",
      "- БиС , дымись канабис , \n",
      "У нас всё ровно , в \n",
      "рот ебись шоубиз , GFR1 . \n",
      "Пройдет пара лет и пара зим \n",
      ", вернешься ко всем моим чувствам \n",
      "! Слух , нюх , осезание \n",
      ", зрение , Уже в полностью \n",
      "в его распоряжении . Что же \n",
      "станет с моими любимыми файлами ? \n",
      "Удовольствие , С бутылочкой колы на \n",
      "лавочке , с трёхлитровым движком , \n",
      "Литые диски , на крыше сиськи \n",
      ", номер , На ручку вешаю \n",
      "табличку «Не беспокоить» , и нам \n",
      "реально похуй кто там вертит мордой \n",
      ", А ну давай , сильно \n",
      "рад сам поразмысли Смешные гонорары , \n",
      "значит я папа нехилый , лимузины \n",
      "там , виллы , Но делать \n",
      "нечего , надо пойти по есть \n",
      "чего - нибудь И лень поставить \n",
      "чайник и в музей , Прогуляйтесь \n",
      "по нашему подземелью . Нет — \n",
      "Как это , но я "
     ]
    }
   ],
   "source": [
    "rap_gad = tg.generate()\n",
    "l = rap_gad.split(' ')\n",
    "for i in range(0, len(l)):\n",
    "    print(l[i], end=' ')\n",
    "    if i % 6 == 0:\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
